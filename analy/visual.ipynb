{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71ffecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133c2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\\\n",
    ".setMaster(\"spark://hadoop-master:7077\")\\\n",
    ".setAppName(\"test Spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33ea447",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4e8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d50d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobid,keyword,title,place,salary,experience,education,companytype,company_name,jobwelf,time\n"
     ]
    }
   ],
   "source": [
    "data = sc.textFile(\"hdfs://hadoop-master:9999/data.csv\")\n",
    "header = data.first()\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44afe775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['132607755',\n",
       "  'C++',\n",
       "  '采销经理（3C）',\n",
       "  '北京',\n",
       "  '1-1.5万/月',\n",
       "  '3-4年经验',\n",
       "  '大专',\n",
       "  '创业公司',\n",
       "  '北京华创盈讯科技有限公司',\n",
       "  '五险一金 弹性工作 通讯补贴 餐饮补贴 年终奖金 13薪 定期体检 员工旅游 绩效奖金',\n",
       "  '06-08']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data.filter(lambda row : row != header)\n",
    "data_fields = data2.map(lambda line : line.split(\",\")).filter(lambda row : len(row)==11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd5fd472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24370541"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataJson = data_fields.map(lambda x:{\"jobid\":x[0],'keyword':x[1],'title':x[2],'place':x[3],'salary':x[4],'experience':x[5],'education':x[6],'companytype':x[7],'company_name':x[8],'jobwelf':x[9],'time':x[10]})\n",
    "open('/home/hadoop/Desktop/data/dataJson','w').write(str(dataJson.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4aee714b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34744"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cityCompanytype = data_fields.map(lambda x : ((x[7],x[3]),1)).reduceByKey(lambda x,y : x+y)\n",
    "cityCompanytype = cityCompanytype.filter(lambda x : not x[0][0] == '')\n",
    "cityCompanytype = cityCompanytype.map(lambda x:{\"city\":x[0][1],'companytype':x[0][0],\"value\":x[1]})\n",
    "open('/home/hadoop/Desktop/data/cityCompanytype.json','w').write(str(cityCompanytype.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a399f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94647"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据总量\n",
    "data_fields.count()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03493682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9957"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 企业个数\n",
    "companysNum = data_fields.map(lambda x : (x[8],1)).reduceByKey(lambda x,y : x+y)\n",
    "companysNum.count()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27ea0e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('创业公司', 2230),\n",
       " ('上市公司', 11191),\n",
       " ('民营公司', 71120),\n",
       " ('外资（欧美）', 1446),\n",
       " ('合资', 3197),\n",
       " ('外资（非欧美）', 1730),\n",
       " ('', 22),\n",
       " ('国企', 3179),\n",
       " ('事业单位', 420),\n",
       " ('非营利组织', 77),\n",
       " ('政府机关', 26),\n",
       " ('外企代表处', 8)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companysNum = data_fields.map(lambda x : (x[7],1)).reduceByKey(lambda x,y : x+y)\n",
    "companysNum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc9d0080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6234"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各城市岗位数量\n",
    "cityJobNum = data_fields.map(lambda x : (x[3],1)).reduceByKey(lambda x,y : x+y)\n",
    "cityJobNum = cityJobNum.map(lambda x:{\"city\":x[0],\"value\":x[1]})\n",
    "open('/home/hadoop/Desktop/data/cityJobNum.json','w').write(str(cityJobNum.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "192b191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combineByKey参数集\n",
    "createCombiner = (lambda value: (value,1))\n",
    "mergeValue = (lambda acc,value: (acc[0]+value,acc[1]+1)) \n",
    "mergeCombiners = (lambda acc1, acc2 : (acc1[0]+acc2[0],acc1[1]+acc2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0029fb1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2181"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不同经验(年)学历的平均工资(万)\n",
    "import re\n",
    "import numpy as np\n",
    "get_unit = lambda x : 10000 if ('万' in x) else(1000 if ('千' in x) else 1)\n",
    "get_time = lambda x : 12 if ('年' in x) else 1\n",
    "get_mean = lambda x: (np.mean(x.astype(np.float64)))\n",
    "get_number = lambda x : np.array(re.findall(r\"\\d+\\.?\\d*\",x))\n",
    "ExperienceDegreeSalary = data_fields.map(lambda x : (x[4],x[5],x[6]))\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.map(lambda x : (round(get_mean(get_number(x[0]).astype(np.float64) * get_unit(x[0])/get_time(x[0]))/10000,1),x[1], x[2]))\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.filter(lambda x : not np.isnan(x[0]) and x[2] in ['高中','本科','大专','中专','博士','硕士'])\n",
    "other = ExperienceDegreeSalary.filter(lambda x : x[1] in '无需经验 在校生/应届生')\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.filter(lambda x : x[1] not in '无需经验 在校生/应届生')\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.map(lambda x : (x[0],int(get_mean(get_number(x[1]).astype(np.float64))), x[2]))\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.filter(lambda x : not np.isnan(x[1]))\n",
    "ExperienceDegreeSalary  = ExperienceDegreeSalary.union(other)\n",
    "\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.map(lambda x:((x[1],x[2]),x[0]))\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.combineByKey(createCombiner,mergeValue,mergeCombiners)\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.map(lambda x:(x[0],round(float(x[1][0])/x[1][1],1)))\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.map(lambda x:(x[0][0],x[0][1],x[1]))\n",
    "ExperienceDegreeSalary = ExperienceDegreeSalary.map(lambda x:{\"experience\":x[0],\"degree\":x[1],\"salary\":x[2]})\n",
    "open('/home/hadoop/Desktop/data/ExperienceDegreeSalary.json','w').write(str(ExperienceDegreeSalary.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "518c8e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42126"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词云\n",
    "import jieba\n",
    "get_seg = lambda x : (jieba.lcut(x, cut_all=False))\n",
    "description = data_fields.flatMap(lambda x : get_seg(x[9]))\n",
    "description = description.map(lambda x : (x,1))\n",
    "description = description.reduceByKey(lambda x,y : x+y).sortBy(lambda x:x[1],False)\n",
    "isChinese = lambda c : '\\u4e00' <= c <= '\\u9fa5'\n",
    "description = description.filter(lambda x : all(map(isChinese,x[0])))\n",
    "description = description.filter(lambda x : len(x[0]) >= 2)\n",
    "description.take(1)\n",
    "description = description.map(lambda x:{\"description\":x[0],\"value\":x[1]})\n",
    "open('/home/hadoop/Desktop/data/description.json','w').write(str(description.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25d95534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2964"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计不同日期平均薪资\n",
    "import re\n",
    "import numpy as np\n",
    "get_unit = lambda x : 10000 if ('万' in x) else(1000 if ('千' in x) else 1)\n",
    "get_time = lambda x : 12 if ('年' in x) else 1\n",
    "get_mean = lambda x,y : (np.mean(x.astype(np.float64) * get_unit(y)/get_time(y)))\n",
    "get_number = lambda x : np.array(re.findall(r\"\\d+\\.?\\d*\",x))\n",
    "daySalary = data_fields.map(lambda x : (x[4],x[10]))\n",
    "daySalary = daySalary.map(lambda x : (x[1], get_mean(get_number(x[0]).astype(np.float64),x[0])))\n",
    "daySalary = daySalary.filter(lambda x : not np.isnan(x[1]))\n",
    "daySalary_sum = daySalary.combineByKey(createCombiner,mergeValue,mergeCombiners)\n",
    "daySalary_mean = daySalary_sum.map(lambda x:(x[0],float(x[1][0])/x[1][1]/10000,x[1][1]))\n",
    "daySalary_mean = daySalary_mean.map(lambda x: (x[0],round((x[1]),1),x[2])).sortBy(lambda x : x[0])\n",
    "daySalary_mean.take(1)\n",
    "daySalary_mean = daySalary_mean.map(lambda x:{\"day\":x[0],\"salary\":x[1],\"value\":x[2]})\n",
    "open('/home/hadoop/Desktop/data/daySalary.json','w').write(str(daySalary_mean.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a3f8cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9458"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各城市平均工资\n",
    "import re\n",
    "import numpy as np\n",
    "get_unit = lambda x : 10000 if ('万' in x) else(1000 if ('千' in x) else 1)\n",
    "get_time = lambda x : 12 if ('年' in x) else 1\n",
    "get_mean = lambda x,y : (np.mean(x.astype(np.float64) * get_unit(y)/get_time(y)))\n",
    "get_number = lambda x : np.array(re.findall(r\"\\d+\\.?\\d*\",x))\n",
    "\n",
    "citySalary = data_fields.map(lambda x : (x[3],x[4]))\n",
    "citySalary = citySalary.map(lambda x : (x[0], get_mean(get_number(x[1]).astype(np.float64),x[1])))\n",
    "citySalary = citySalary.filter(lambda x : not np.isnan(x[1]))\n",
    "citySalary_sum = citySalary.combineByKey(createCombiner,mergeValue,mergeCombiners)\n",
    "citySalary_mean = citySalary_sum.map(lambda x:(x[0],float(x[1][0])/x[1][1]/10000,x[1][1]))\n",
    "citySalary_mean = citySalary_mean.map(lambda x: (x[0],round((x[1]),1),x[2])).sortBy(lambda x : x[0])\n",
    "citySalary_mean.take(1)\n",
    "citySalary_mean = citySalary_mean.map(lambda x:{\"city\":x[0],\"salary\":x[1],\"value\":x[2]})\n",
    "open('/home/hadoop/Desktop/data/citySalary.json','w').write(str(citySalary_mean.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "337bd946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4032"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywordNum = data_fields.map(lambda x : (x[1],1)).reduceByKey(lambda x,y : x+y)\n",
    "keywordNum = keywordNum.map(lambda x:{\"keyword\":x[0],\"value\":x[1]})\n",
    "open('/home/hadoop/Desktop/data/keywordNum.json','w').write(str(keywordNum.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e132116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
